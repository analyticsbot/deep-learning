# Machine Learning & Deep Learning Interview Preparation App

This app is built using Streamlit and provides interactive coding and multiple-choice question practice for machine learning and deep learning topics. It is designed to help interview candidates practice key concepts, algorithms, and techniques by generating coding challenges and questions on-demand.

## Features

- **Coding Practice**
  - Choose from a list of top ML and DL algorithms and concepts (e.g., Linear Regression, K-Means, Self Attention).
  - Set a coding intensity level to determine how much of the code you want to write yourself (from 0% to 100%).
  - Choose to implement the algorithm from scratch or using standard libraries (e.g., `scikit-learn` for machine learning algorithms).
  - Receive generated code from a Large Language Model (LLM) that matches the selected intensity level.
  - Edit, run, and test the code directly in the Streamlit app, mimicking an IDE-like experience.

- **Multiple Choice Questions**
  - Select the number of questions and difficulty level (easy, medium, or hard).
  - Practice answering multiple-choice questions generated by an LLM on the selected topic.
  - Get instant feedback on answers with color-coded results:
    - **Green** for correct answers.
    - **Red** for incorrect answers.
  - Review your final score at the end of each quiz.

## Installation

To set up and run this app locally, follow these steps:

1. **Clone the repository**:

    ```bash
    git clone https://github.com/analyticsbot/deep-learning.git
    cd ai-ml-code-interviewer
    ```

2. **Install dependencies**:

    Make sure you have Python 3.7+ installed. Install the required packages:

    ```bash
    pip install -r requirements.txt
    ```

3. **Set up LLM API**:

   This app requires an LLM (e.g., Llama model served locally) for generating code and questions. Update the API configuration in the app if needed:
   
   ```python
   # API configuration in the code:
   # Replace `base_url` with your local server endpoint and `api_key` if required.
   from openai import OpenAI

   client = OpenAI(base_url="http://localhost:1234/v1", api_key="lm-studio")

## Run the Streamlit app:
```bash
streamlit run ml_interview_app.py
```

## Usage
1. Coding Practice
    - Navigate to the Coding tab.
    - Select a topic from the dropdown menu to focus on a specific algorithm or concept.
    - Adjust the Coding Intensity slider from 0% to 100%.
        - 0%: Full code is provided by the LLM.
        - 100%: A skeleton code structure is provided for complete implementation by the user.
        - Intermediate values give partial code with coding prompts.
    - Choose whether you want to write the code From Scratch or Using Standard Package (e.g., scikit-learn).
    - Click Get Code to receive code suggestions from the LLM based on your selections.
    - Edit the code as needed in the text area, and click Run Code to execute the code and see the results.

2. Multiple Choice Questions
    - Navigate to the Multiple Choice Questions tab.
    - Set the number of questions you want to answer and choose the difficulty level.
    - Click Get Questions to generate multiple-choice questions from the LLM.
    - Select your answers and click Submit Answers.
    - After submission, answers are checked against the correct ones:
        - Green text indicates correct answers.
        - Red text indicates incorrect answers.
    - A final score is displayed, showing the total number of correct answers.

## Example Usage
- Example Coding Task:

    - Select "Linear Regression" as the topic.
    - Set Coding Intensity to 50%.
    - Choose Using Standard Package.
    - Click Get Code to receive partially completed code, edit it as desired, and run it in the app.
- Example Quiz:

    - Select "5" as the number of questions.
    - Choose "Medium" for difficulty.
    - Click Get Questions to start the quiz, answer the questions, and view feedback.

## Dependencies
    - Streamlit: Interactive UI for Python applications.
    - Requests: API calls to the LLM server.
    - OpenAI API: Accessing the local LLM (Meta Llama in this case).
    - Note: Check requirements.txt for other dependencies if additional libraries are used.

## Troubleshooting
LLM Server Not Responding:

    - Ensure the LLM server is running locally at the endpoint specified in the API configuration.
    - Check for network/firewall restrictions that may block the API request.

Code Execution Errors:

    - Ensure your code does not contain syntax or logical errors before running.
    - Check the console log for detailed error messages.

Improper Answer Feedback in Multiple Choice Questions:

    - Ensure that the question format from the LLM response is correctly parsed.

## Contributing
Contributions are welcome! Please fork the repository and submit a pull request for any suggestions or improvements.
