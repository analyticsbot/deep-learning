# Machine Learning & Deep Learning Interview Preparation App

This app is built using Streamlit and provides interactive coding and multiple-choice question practice for machine learning and deep learning topics. It is designed to help interview candidates practice key concepts, algorithms, and techniques by generating coding challenges and questions on-demand.

## Features

- **Coding Practice**
  - Choose from a list of top ML and DL algorithms and concepts (e.g., Linear Regression, K-Means, Self Attention).
  - Set a coding intensity level to determine how much of the code you want to write yourself (from 0% to 100%).
  - Choose to implement the algorithm from scratch or using standard libraries (e.g., `scikit-learn` for machine learning algorithms).
  - Receive generated code from a Large Language Model (LLM) that matches the selected intensity level.
  - Edit, run, and test the code directly in the Streamlit app, mimicking an IDE-like experience.

- **Multiple Choice Questions**
  - Select the number of questions and difficulty level (easy, medium, or hard).
  - Practice answering multiple-choice questions generated by an LLM on the selected topic.
  - Get instant feedback on answers with color-coded results.
  - Review your final score at the end of each quiz.
  - Request explanations for questions to deepen your understanding.

- **New Features**
  - Improved code syntax highlighting
  - Safer code execution environment
  - Better error handling for LLM responses
  - Configurable settings via environment variables
  - Enhanced UI with responsive design

## Project Structure

```
ai-ml-code-interviewer/
├── app.py                 # Main application entry point
├── coding_module.py       # Coding practice functionality
├── quiz_module.py         # Quiz functionality
├── llm_service.py         # LLM API interaction service
├── code_executor.py       # Safe code execution environment
├── utils.py               # Utility functions
├── config.py              # Configuration settings
├── requirements.txt       # Project dependencies
├── .env.example           # Example environment variables
└── Readme.md              # Project documentation
```

## Installation

To set up and run this app locally, follow these steps:

1. **Clone the repository**:

    ```bash
    git clone https://github.com/analyticsbot/deep-learning.git
    cd ai-ml-code-interviewer
    ```

2. **Install dependencies**:

    Make sure you have Python 3.7+ installed. Install the required packages:

    ```bash
    pip install -r requirements.txt
    ```

3. **Set up environment variables**:

   Copy the `.env.example` file to `.env` and update the settings as needed:

   ```bash
   cp .env.example .env
   ```

   Edit the `.env` file to configure your LLM API settings:

   ```
   LLM_BASE_URL=http://localhost:1234/v1
   LLM_API_KEY=lm-studio
   LLM_MODEL=lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF
   LLM_TEMPERATURE=0.7
   ```

4. **Run the Streamlit app**:

   ```bash
   streamlit run app.py
   ```

## Usage

### Coding Practice

1. Navigate to the "Coding Practice" tab
2. Select a topic from the dropdown menu (e.g., "Linear Regression")
3. Adjust the coding intensity slider (0% for full code, 100% for skeleton)
4. Choose implementation approach ("From Scratch" or "Using Standard Package")
5. Click "Get Code" to generate code from the LLM
6. Edit the code in the text area
7. Click "Run Code" to execute and see results
8. Use "Explain This Code" to get a detailed explanation

### Multiple Choice Questions

1. Navigate to the "Multiple Choice Questions" tab
2. Select a topic, number of questions, and difficulty level
3. Click "Get Questions" to generate a quiz
4. Select your answers for each question
5. Click "Submit Answers" to check your results
6. Review your score and correct/incorrect answers
7. Click "Explain" buttons to get explanations for specific questions
8. Use "Start New Quiz" to generate a new set of questions

### Settings

The "Settings" tab allows you to:

- View current LLM configuration
- Toggle code execution (for security reasons)
- Learn more about the application

## Security Features

- Code execution is disabled by default for security (can be enabled in settings)
- Restricted execution environment when running user code
- Input validation and sanitization for LLM prompts
- Error handling for LLM API calls

## Dependencies

- `streamlit`: Interactive UI framework
- `openai`: OpenAI-compatible API client
- `python-dotenv`: Environment variable management
- `requests`: HTTP requests
- `pydantic`: Data validation
- `markdown`: Markdown processing
- `pygments`: Code syntax highlighting

## Troubleshooting

### LLM Server Issues

- Ensure your LLM server is running at the configured endpoint
- Check network connectivity and firewall settings
- Verify API key if authentication is required

### Code Execution Problems

- Make sure code execution is enabled in settings if you want to run code
- Check for syntax errors in your code
- Some libraries may not be available in the restricted execution environment

### UI Issues

- Try refreshing the page if UI elements are not responding
- Check Streamlit version compatibility

## Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes
4. Commit your changes (`git commit -m 'Add some amazing feature'`)
5. Push to the branch (`git push origin feature/amazing-feature`)
6. Open a Pull Request

## License

This project is licensed under the MIT License - see the LICENSE file for details.
