{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/IDEA-Research/GroundingDINO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/wxWDt5UiwY8\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x103b08820>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "# Specify the video ID\n",
    "video_id = \"wxWDt5UiwY8\"\n",
    "\n",
    "# Embed the video\n",
    "YouTubeVideo(video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rshankar/Downloads/Projects/new_folder/deep-learning/ml-dl-experiments/notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rshankar/Downloads/Projects/new_folder/deep-learning/ml-dl-experiments/notebooks\n",
      "Cloning into 'GroundingDINO'...\n",
      "remote: Enumerating objects: 463, done.\u001b[K\n",
      "remote: Counting objects: 100% (240/240), done.\u001b[K\n",
      "remote: Compressing objects: 100% (104/104), done.\u001b[K\n",
      "remote: Total 463 (delta 175), reused 136 (delta 136), pack-reused 223 (from 1)\u001b[K\n",
      "Receiving objects: 100% (463/463), 12.87 MiB | 23.00 MiB/s, done.\n",
      "Resolving deltas: 100% (241/241), done.\n",
      "/Users/rshankar/Downloads/Projects/new_folder/deep-learning/ml-dl-experiments/notebooks/GroundingDINO\n"
     ]
    }
   ],
   "source": [
    "%cd {HOME}\n",
    "!git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
    "%cd {HOME}/GroundingDINO\n",
    "!pip install -q -e .\n",
    "!pip install -q roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rshankar/Downloads/Projects/new_folder/deep-learning/ml-dl-experiments/notebooks/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py ; exist: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "CONFIG_PATH = os.path.join(HOME, \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\")\n",
    "print(CONFIG_PATH, \"; exist:\", os.path.isfile(CONFIG_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rshankar/Downloads/Projects/new_folder/deep-learning/ml-dl-experiments/notebooks\n",
      "/Users/rshankar/Downloads/Projects/new_folder/deep-learning/ml-dl-experiments/notebooks/weights\n"
     ]
    }
   ],
   "source": [
    "%cd {HOME}\n",
    "!mkdir {HOME}/weights\n",
    "%cd {HOME}/weights\n",
    "\n",
    "!wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rshankar/Downloads/Projects/new_folder/deep-learning/ml-dl-experiments/notebooks/weights/groundingdino_swint_ogc.pth ; exist: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "WEIGHTS_NAME = \"groundingdino_swint_ogc.pth\"\n",
    "WEIGHTS_PATH = os.path.join(HOME, \"weights\", WEIGHTS_NAME)\n",
    "print(WEIGHTS_PATH, \"; exist:\", os.path.isfile(WEIGHTS_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rshankar/Downloads/Projects/new_folder/deep-learning/ml-dl-experiments/notebooks\n",
      "/Users/rshankar/Downloads/Projects/new_folder/deep-learning/ml-dl-experiments/notebooks/data\n"
     ]
    }
   ],
   "source": [
    "%cd {HOME}\n",
    "!mkdir {HOME}/data\n",
    "%cd {HOME}/data\n",
    "\n",
    "!wget -q https://media.roboflow.com/notebooks/examples/dog.jpeg\n",
    "!wget -q https://media.roboflow.com/notebooks/examples/dog-2.jpeg\n",
    "!wget -q https://media.roboflow.com/notebooks/examples/dog-3.jpeg\n",
    "!wget -q https://media.roboflow.com/notebooks/examples/dog-4.jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rshankar/Downloads/Projects/new_folder/deep-learning/ml-dl-experiments/notebooks/GroundingDINO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Failed to load image Python extension: 'dlopen(/Users/rshankar/anaconda3/envs/cv/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <5AA8DD3D-A2CC-31CA-8060-88B4E9C18B09> /Users/rshankar/anaconda3/envs/cv/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Expected in:     <EEB3232B-F6A7-3262-948C-BB2F54905803> /Users/rshankar/anaconda3/envs/cv/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "UserWarning: Failed to load custom C++ ops. Running on CPU mode Only!\n",
      "UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /private/var/folders/k1/30mswbxs7r1g6zwn8y4fyt500000gp/T/abs_f2vdv9nc2o/croot/pytorch-select_1725908300995/work/aten/src/ATen/native/TensorShape.cpp:3588.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "%cd {HOME}/GroundingDINO\n",
    "\n",
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "\n",
    "model = load_model(CONFIG_PATH, WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;66;03m# if isinstance(image, torch.Tensor) else image\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Run prediction\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m boxes, logits, phrases \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcaption\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTEXT_PROMPT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbox_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBOX_TRESHOLD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTEXT_TRESHOLD\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Annotate image\u001b[39;00m\n\u001b[1;32m     36\u001b[0m annotated_frame \u001b[38;5;241m=\u001b[39m annotate(image_source\u001b[38;5;241m=\u001b[39mimage_source, boxes\u001b[38;5;241m=\u001b[39mboxes, logits\u001b[38;5;241m=\u001b[39mlogits, phrases\u001b[38;5;241m=\u001b[39mphrases)\n",
      "File \u001b[0;32m~/Downloads/Projects/new_folder/deep-learning/ml-dl-experiments/notebooks/GroundingDINO/groundingdino/util/inference.py:64\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(model, image, caption, box_threshold, text_threshold, device, remove_combined)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m     54\u001b[0m         model,\n\u001b[1;32m     55\u001b[0m         image: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m         remove_combined: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     61\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor, List[\u001b[38;5;28mstr\u001b[39m]]:\n\u001b[1;32m     62\u001b[0m     caption \u001b[38;5;241m=\u001b[39m preprocess_caption(caption\u001b[38;5;241m=\u001b[39mcaption)\n\u001b[0;32m---> 64\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.10/site-packages/torch/nn/modules/module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 779 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.10/site-packages/torch/nn/modules/module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.10/site-packages/torch/nn/modules/module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1154\u001b[0m             device,\n\u001b[1;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1156\u001b[0m             non_blocking,\n\u001b[1;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1158\u001b[0m         )\n\u001b[0;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.10/site-packages/torch/cuda/__init__.py:284\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "import supervision as sv\n",
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# Ensure HOME is defined properly\n",
    "HOME = os.path.expanduser(\"~\")\n",
    "\n",
    "# Paths and Parameters\n",
    "IMAGE_NAME = \"/Users/rshankar/Downloads/Projects/new_folder/deep-learning/ml-dl-experiments/notebooks/data/dog-3.jpeg\"\n",
    "IMAGE_PATH = os.path.join(HOME, \"data\", IMAGE_NAME)\n",
    "TEXT_PROMPT = \"chair\"\n",
    "BOX_TRESHOLD = 0.35\n",
    "TEXT_TRESHOLD = 0.25\n",
    "\n",
    "# Load model and move to CPU\n",
    "model = load_model(\"groundingdino/config/GroundingDINO_SwinT_OGC.py\", \"/Users/rshankar/Downloads/Projects/new_folder/deep-learning/ml-dl-experiments/notebooks/weights/groundingdino_swint_ogc.pth\")\n",
    "model = model.to('cpu')  # Force the model to run on CPU\n",
    "\n",
    "# Load image\n",
    "image_source, image = load_image(IMAGE_PATH)\n",
    "\n",
    "# Ensure the image tensor is on CPU if required\n",
    "image = image.to('cpu')# if isinstance(image, torch.Tensor) else image\n",
    "\n",
    "# Run prediction\n",
    "boxes, logits, phrases = predict(\n",
    "    model=model, \n",
    "    image=image, \n",
    "    caption=TEXT_PROMPT, \n",
    "    box_threshold=BOX_TRESHOLD, \n",
    "    text_threshold=TEXT_TRESHOLD\n",
    ")\n",
    "\n",
    "# Annotate image\n",
    "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "\n",
    "# Plot the result\n",
    "%matplotlib inline\n",
    "sv.plot_image(annotated_frame, (16, 16))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
