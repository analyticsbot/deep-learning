{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import speech_recognition as sr\n",
    "import requests\n",
    "import random\n",
    "import textstat\n",
    "from openai import OpenAI  # Importing OpenAI to connect with LM Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make LLM call\n",
    "def call_llm(prompt, model=\"lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF\"):\n",
    "    client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an intelligent assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        stream=True,\n",
    "    )\n",
    "    response_text = \"\"\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            response_text += chunk.choices[0].delta.content\n",
    "    return response_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_text = \"\"\"• Live in the future: Let’s build the future of work that we want, with an in-person focus designed to support a strong, valuable experience for our people who have chosen to work from the office, and a thoughtful and intentional approach to where we invest in remote work. This also means being early adopters of the future products we build to help people feel present together wherever they are.\n",
    "\"\"\"\n",
    "behavior_role = \"Define the role of the LLM (e.g., HR recruiter):\", \"HR recruiter\"\n",
    "response_criteria = \"\"\"Describe the type of response needed (e.g., brevity, time taken, quality):\", Provide feedback on time taken, brevity, filler words, repetition, and answer quality.\"\"\"\n",
    "job_role = 'ML Manager'\n",
    "company_name = \"Meta\"\n",
    "# Call the LLM for feedback\n",
    "feedback = call_llm(\n",
    "    f\"As a {behavior_role} at {company_name}, interviewing for {job_role}, analyze this answer: '{response_text}'. Focus on: {response_criteria}. Keep the response brief and use bullet points\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s my analysis:\\n\\n**Time taken:** 5-7 seconds to read and process\\n\\n**Brevity:**\\n\\n* Answer is concise (1 sentence) but lacks depth\\n* Could be more specific about what \"future products\" mean in this context\\n\\n**Filler words:** None notable, good job!\\n\\n**Repetition:** None notable, good job!\\n\\n**Answer quality:**\\n\\n* Shows enthusiasm and vision for the future of work at Meta\\n* Addresses a key aspect (hybrid work) but lacks concrete examples or tangible goals\\n* Could benefit from more specific details about what it means to \"build the future\" in this context\\n\\nOverall, the response is brief and shows some creativity, but could benefit from more substance and specificity.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
