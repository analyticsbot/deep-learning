{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "from keras import activations\n",
    "from keras import utils\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the squashing function.\n",
    "# we use 0.5 in stead of 1 in hinton's paper.\n",
    "# if 1, the norm of vector will be zoomed out.\n",
    "# if 0.5, the norm will be zoomed in while original norm is less than 0.5\n",
    "# and be zoomed out while original norm is greater than 0.5.\n",
    "def squash(x, axis=-1):\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    scale = K.sqrt(s_squared_norm) / (0.5 + s_squared_norm)\n",
    "    return scale * x\n",
    "\n",
    "\n",
    "# define our own softmax function instead of K.softmax\n",
    "# because K.softmax can not specify axis.\n",
    "def softmax(x, axis=-1):\n",
    "    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "    return ex / K.sum(ex, axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "# define the margin loss like hinge loss\n",
    "def margin_loss(y_true, y_pred):\n",
    "    lamb, margin = 0.5, 0.1\n",
    "    return K.sum(y_true * K.square(K.relu(1 - margin - y_pred)) + lamb * (\n",
    "        1 - y_true) * K.square(K.relu(y_pred - margin)), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Capsule(Layer):\n",
    "    \"\"\"A Capsule Implement with Pure Keras\n",
    "    There are two vesions of Capsule.\n",
    "    One is like dense layer (for the fixed-shape input),\n",
    "    and the other is like timedistributed dense (for various length input).\n",
    "    The input shape of Capsule must be (batch_size,\n",
    "                                        input_num_capsule,\n",
    "                                        input_dim_capsule\n",
    "                                       )\n",
    "    and the output shape is (batch_size,\n",
    "                             num_capsule,\n",
    "                             dim_capsule\n",
    "                            )\n",
    "    Capsule Implement is from https://github.com/bojone/Capsule/\n",
    "    Capsule Paper: https://arxiv.org/abs/1710.09829\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_capsule,\n",
    "                 dim_capsule,\n",
    "                 routings=3,\n",
    "                 share_weights=True,\n",
    "                 activation='squash',\n",
    "                 **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'squash':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = activations.get(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.kernel = self.add_weight(\n",
    "                name='capsule_kernel',\n",
    "                shape=(1, input_dim_capsule,\n",
    "                       self.num_capsule * self.dim_capsule),\n",
    "                initializer='glorot_uniform',\n",
    "                trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.kernel = self.add_weight(\n",
    "                name='capsule_kernel',\n",
    "                shape=(input_num_capsule, input_dim_capsule,\n",
    "                       self.num_capsule * self.dim_capsule),\n",
    "                initializer='glorot_uniform',\n",
    "                trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Following the routing algorithm from Hinton's paper,\n",
    "        but replace b = b + <u,v> with b = <u,v>.\n",
    "        This change can improve the feature representation of Capsule.\n",
    "        However, you can replace\n",
    "            b = K.batch_dot(outputs, hat_inputs, [2, 3])\n",
    "        with\n",
    "            b += K.batch_dot(outputs, hat_inputs, [2, 3])\n",
    "        to realize a standard routing.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.share_weights:\n",
    "            hat_inputs = K.conv1d(inputs, self.kernel)\n",
    "        else:\n",
    "            hat_inputs = K.local_conv1d(inputs, self.kernel, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(inputs)[0]\n",
    "        input_num_capsule = K.shape(inputs)[1]\n",
    "        hat_inputs = K.reshape(hat_inputs,\n",
    "                               (batch_size, input_num_capsule,\n",
    "                                self.num_capsule, self.dim_capsule))\n",
    "        hat_inputs = K.permute_dimensions(hat_inputs, (0, 2, 1, 3))\n",
    "\n",
    "        b = K.zeros_like(hat_inputs[:, :, :, 0])\n",
    "        for i in range(self.routings):\n",
    "            c = softmax(b, 1)\n",
    "            if K.backend() == 'theano':\n",
    "                o = K.sum(o, axis=1)\n",
    "            o = self.activation(K.batch_dot(c, hat_inputs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = K.batch_dot(o, hat_inputs, [2, 3])\n",
    "                if K.backend() == 'theano':\n",
    "                    o = K.sum(o, axis=1)\n",
    "\n",
    "        return o\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A common Conv2D model\n",
    "input_image = Input(shape=(None, None, 3))\n",
    "x = Conv2D(64, (3, 3), activation='relu')(input_image)\n",
    "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = AveragePooling2D((2, 2))(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "capsule_1 (Capsule)          (None, 10, 16)            20480     \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 280,640\n",
      "Trainable params: 280,640\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"now we reshape it as (batch_size, input_num_capsule, input_dim_capsule)\n",
    "then connect a Capsule layer.\n",
    "the output of final model is the lengths of 10 Capsule, whose dim=16.\n",
    "the length of Capsule is the proba,\n",
    "so the problem becomes a 10 two-classification problem.\n",
    "\"\"\"\n",
    "\n",
    "x = Reshape((-1, 128))(x)\n",
    "capsule = Capsule(10, 16, 3, True)(x)\n",
    "output = Lambda(lambda z: K.sqrt(K.sum(K.square(z), 2)))(capsule)\n",
    "model = Model(inputs=input_image, outputs=output)\n",
    "\n",
    "# we use a margin loss\n",
    "model.compile(loss=margin_loss, optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can compare the performance with or without data augmentation\n",
    "data_augmentation = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(4*60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "Epoch 1/100\n",
      "391/391 [==============================] - 1169s 3s/step - loss: 0.4333 - acc: 0.3306 - val_loss: 0.3746 - val_acc: 0.4477\n",
      "Epoch 2/100\n",
      "391/391 [==============================] - 1173s 3s/step - loss: 0.3545 - acc: 0.4810 - val_loss: 0.3384 - val_acc: 0.4958\n",
      "Epoch 3/100\n",
      "391/391 [==============================] - 1176s 3s/step - loss: 0.3096 - acc: 0.5609 - val_loss: 0.3025 - val_acc: 0.5675\n",
      "Epoch 4/100\n",
      "391/391 [==============================] - 1173s 3s/step - loss: 0.2792 - acc: 0.6152 - val_loss: 0.2614 - val_acc: 0.6409\n",
      "Epoch 5/100\n",
      "391/391 [==============================] - 1191s 3s/step - loss: 0.2539 - acc: 0.6541 - val_loss: 0.2377 - val_acc: 0.6810\n",
      "Epoch 6/100\n",
      "391/391 [==============================] - 1180s 3s/step - loss: 0.2373 - acc: 0.6815 - val_loss: 0.2273 - val_acc: 0.6953\n",
      "Epoch 7/100\n",
      "391/391 [==============================] - 1169s 3s/step - loss: 0.2240 - acc: 0.7042 - val_loss: 0.2163 - val_acc: 0.7165\n",
      "Epoch 8/100\n",
      "391/391 [==============================] - 1180s 3s/step - loss: 0.2145 - acc: 0.7178 - val_loss: 0.2138 - val_acc: 0.7172\n",
      "Epoch 9/100\n",
      "391/391 [==============================] - 1172s 3s/step - loss: 0.2035 - acc: 0.7341 - val_loss: 0.1953 - val_acc: 0.7433\n",
      "Epoch 10/100\n",
      "391/391 [==============================] - 1168s 3s/step - loss: 0.1955 - acc: 0.7489 - val_loss: 0.1980 - val_acc: 0.7413\n",
      "Epoch 11/100\n",
      "391/391 [==============================] - 1206s 3s/step - loss: 0.1886 - acc: 0.7599 - val_loss: 0.1873 - val_acc: 0.7622\n",
      "Epoch 12/100\n",
      "391/391 [==============================] - 1377s 4s/step - loss: 0.1839 - acc: 0.7662 - val_loss: 0.1830 - val_acc: 0.7648\n",
      "Epoch 13/100\n",
      "391/391 [==============================] - 1370s 4s/step - loss: 0.1777 - acc: 0.7763 - val_loss: 0.1804 - val_acc: 0.7714\n",
      "Epoch 14/100\n",
      "391/391 [==============================] - 1403s 4s/step - loss: 0.1712 - acc: 0.7858 - val_loss: 0.1785 - val_acc: 0.7728\n",
      "Epoch 15/100\n",
      "391/391 [==============================] - 1449s 4s/step - loss: 0.1673 - acc: 0.7939 - val_loss: 0.1727 - val_acc: 0.7848\n",
      "Epoch 16/100\n",
      "391/391 [==============================] - 1600s 4s/step - loss: 0.1635 - acc: 0.7978 - val_loss: 0.1724 - val_acc: 0.7835\n",
      "Epoch 17/100\n",
      "391/391 [==============================] - 1354s 3s/step - loss: 0.1601 - acc: 0.8059 - val_loss: 0.1774 - val_acc: 0.7794\n",
      "Epoch 18/100\n",
      "391/391 [==============================] - 1473s 4s/step - loss: 0.1558 - acc: 0.8120 - val_loss: 0.1717 - val_acc: 0.7900\n",
      "Epoch 19/100\n",
      "391/391 [==============================] - 1329s 3s/step - loss: 0.1531 - acc: 0.8153 - val_loss: 0.1788 - val_acc: 0.7696\n",
      "Epoch 20/100\n",
      "391/391 [==============================] - 1379s 4s/step - loss: 0.1499 - acc: 0.8206 - val_loss: 0.1642 - val_acc: 0.7970\n",
      "Epoch 21/100\n",
      "391/391 [==============================] - 1374s 4s/step - loss: 0.1475 - acc: 0.8263 - val_loss: 0.1656 - val_acc: 0.7855\n",
      "Epoch 22/100\n",
      "391/391 [==============================] - 999s 3s/step - loss: 0.1447 - acc: 0.8264 - val_loss: 0.1566 - val_acc: 0.8056\n",
      "Epoch 23/100\n",
      "391/391 [==============================] - 772s 2s/step - loss: 0.1414 - acc: 0.8316 - val_loss: 0.1680 - val_acc: 0.7970\n",
      "Epoch 24/100\n",
      "391/391 [==============================] - 676s 2s/step - loss: 0.1397 - acc: 0.8362 - val_loss: 0.1656 - val_acc: 0.7893\n",
      "Epoch 25/100\n",
      "391/391 [==============================] - 762s 2s/step - loss: 0.1383 - acc: 0.8382 - val_loss: 0.1633 - val_acc: 0.7991\n",
      "Epoch 26/100\n",
      "391/391 [==============================] - 776s 2s/step - loss: 0.1344 - acc: 0.8432 - val_loss: 0.1581 - val_acc: 0.8050\n",
      "Epoch 27/100\n",
      "391/391 [==============================] - 806s 2s/step - loss: 0.1322 - acc: 0.8467 - val_loss: 0.1569 - val_acc: 0.8070\n",
      "Epoch 28/100\n",
      "391/391 [==============================] - 775s 2s/step - loss: 0.1312 - acc: 0.8496 - val_loss: 0.1592 - val_acc: 0.8035\n",
      "Epoch 29/100\n",
      "391/391 [==============================] - 924s 2s/step - loss: 0.1283 - acc: 0.8552 - val_loss: 0.1527 - val_acc: 0.8179\n",
      "Epoch 30/100\n",
      "391/391 [==============================] - 860s 2s/step - loss: 0.1265 - acc: 0.8572 - val_loss: 0.1543 - val_acc: 0.8100\n",
      "Epoch 31/100\n",
      "391/391 [==============================] - 809s 2s/step - loss: 0.1255 - acc: 0.8570 - val_loss: 0.1613 - val_acc: 0.8020\n",
      "Epoch 32/100\n",
      "391/391 [==============================] - 813s 2s/step - loss: 0.1243 - acc: 0.8600 - val_loss: 0.1545 - val_acc: 0.8133\n",
      "Epoch 33/100\n",
      "391/391 [==============================] - 827s 2s/step - loss: 0.1221 - acc: 0.8644 - val_loss: 0.1559 - val_acc: 0.8096\n",
      "Epoch 34/100\n",
      "391/391 [==============================] - 680s 2s/step - loss: 0.1196 - acc: 0.8656 - val_loss: 0.1565 - val_acc: 0.8134\n",
      "Epoch 35/100\n",
      "391/391 [==============================] - 642s 2s/step - loss: 0.1192 - acc: 0.8673 - val_loss: 0.1535 - val_acc: 0.8108\n",
      "Epoch 36/100\n",
      "391/391 [==============================] - 633s 2s/step - loss: 0.1187 - acc: 0.8678 - val_loss: 0.1541 - val_acc: 0.8144\n",
      "Epoch 37/100\n",
      "391/391 [==============================] - 646s 2s/step - loss: 0.1155 - acc: 0.8731 - val_loss: 0.1567 - val_acc: 0.8117\n",
      "Epoch 38/100\n",
      "391/391 [==============================] - 619s 2s/step - loss: 0.1156 - acc: 0.8732 - val_loss: 0.1599 - val_acc: 0.8011\n",
      "Epoch 39/100\n",
      "391/391 [==============================] - 729s 2s/step - loss: 0.1141 - acc: 0.8750 - val_loss: 0.1498 - val_acc: 0.8178\n",
      "Epoch 40/100\n",
      "391/391 [==============================] - 2554s 7s/step - loss: 0.1130 - acc: 0.8761 - val_loss: 0.1549 - val_acc: 0.8097\n",
      "Epoch 41/100\n",
      "391/391 [==============================] - 7617s 19s/step - loss: 0.1118 - acc: 0.8792 - val_loss: 0.1526 - val_acc: 0.8168\n",
      "Epoch 42/100\n",
      "391/391 [==============================] - 608s 2s/step - loss: 0.1100 - acc: 0.8816 - val_loss: 0.1535 - val_acc: 0.8166\n",
      "Epoch 43/100\n",
      "391/391 [==============================] - 597s 2s/step - loss: 0.1087 - acc: 0.8844 - val_loss: 0.1605 - val_acc: 0.7964\n",
      "Epoch 44/100\n",
      "391/391 [==============================] - 592s 2s/step - loss: 0.1074 - acc: 0.8862 - val_loss: 0.1543 - val_acc: 0.8126\n",
      "Epoch 45/100\n",
      "391/391 [==============================] - 593s 2s/step - loss: 0.1068 - acc: 0.8880 - val_loss: 0.1519 - val_acc: 0.8160\n",
      "Epoch 46/100\n",
      "391/391 [==============================] - 599s 2s/step - loss: 0.1058 - acc: 0.8890 - val_loss: 0.1480 - val_acc: 0.8268\n",
      "Epoch 47/100\n",
      "391/391 [==============================] - 614s 2s/step - loss: 0.1050 - acc: 0.8899 - val_loss: 0.1503 - val_acc: 0.8156\n",
      "Epoch 48/100\n",
      "391/391 [==============================] - 634s 2s/step - loss: 0.1043 - acc: 0.8910 - val_loss: 0.1591 - val_acc: 0.8068\n",
      "Epoch 49/100\n",
      "391/391 [==============================] - 654s 2s/step - loss: 0.1036 - acc: 0.8926 - val_loss: 0.1456 - val_acc: 0.8228\n",
      "Epoch 50/100\n",
      "391/391 [==============================] - 718s 2s/step - loss: 0.1023 - acc: 0.8945 - val_loss: 0.1504 - val_acc: 0.8186\n",
      "Epoch 51/100\n",
      "391/391 [==============================] - 669s 2s/step - loss: 0.1017 - acc: 0.8956 - val_loss: 0.1606 - val_acc: 0.8153\n",
      "Epoch 52/100\n",
      "391/391 [==============================] - 599s 2s/step - loss: 0.1005 - acc: 0.8972 - val_loss: 0.1509 - val_acc: 0.8145\n",
      "Epoch 53/100\n",
      "391/391 [==============================] - 643s 2s/step - loss: 0.0999 - acc: 0.8986 - val_loss: 0.1550 - val_acc: 0.8212\n",
      "Epoch 54/100\n",
      "391/391 [==============================] - 674s 2s/step - loss: 0.0991 - acc: 0.8982 - val_loss: 0.1495 - val_acc: 0.8213\n",
      "Epoch 55/100\n",
      "391/391 [==============================] - 671s 2s/step - loss: 0.0980 - acc: 0.9005 - val_loss: 0.1545 - val_acc: 0.8156\n",
      "Epoch 56/100\n",
      "391/391 [==============================] - 680s 2s/step - loss: 0.0980 - acc: 0.8998 - val_loss: 0.1491 - val_acc: 0.8211\n",
      "Epoch 57/100\n",
      "391/391 [==============================] - 603s 2s/step - loss: 0.0960 - acc: 0.9053 - val_loss: 0.1521 - val_acc: 0.8229\n",
      "Epoch 58/100\n",
      "391/391 [==============================] - 594s 2s/step - loss: 0.0950 - acc: 0.9051 - val_loss: 0.1570 - val_acc: 0.8170\n",
      "Epoch 59/100\n",
      "391/391 [==============================] - 1247s 3s/step - loss: 0.0952 - acc: 0.9067 - val_loss: 0.1486 - val_acc: 0.8272\n",
      "Epoch 60/100\n",
      "391/391 [==============================] - 1719s 4s/step - loss: 0.0948 - acc: 0.9041 - val_loss: 0.1510 - val_acc: 0.8171\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 760s 2s/step - loss: 0.0938 - acc: 0.9071 - val_loss: 0.1484 - val_acc: 0.8285\n",
      "Epoch 62/100\n",
      "391/391 [==============================] - 731s 2s/step - loss: 0.0924 - acc: 0.9108 - val_loss: 0.1575 - val_acc: 0.8186\n",
      "Epoch 63/100\n",
      "391/391 [==============================] - 746s 2s/step - loss: 0.0925 - acc: 0.9092 - val_loss: 0.1504 - val_acc: 0.8262\n",
      "Epoch 64/100\n",
      "391/391 [==============================] - 702s 2s/step - loss: 0.0920 - acc: 0.9104 - val_loss: 0.1514 - val_acc: 0.8187\n",
      "Epoch 65/100\n",
      "391/391 [==============================] - 683s 2s/step - loss: 0.0916 - acc: 0.9113 - val_loss: 0.1616 - val_acc: 0.8071\n",
      "Epoch 66/100\n",
      "391/391 [==============================] - 805s 2s/step - loss: 0.0901 - acc: 0.9126 - val_loss: 0.1553 - val_acc: 0.8183\n",
      "Epoch 67/100\n",
      "391/391 [==============================] - 712s 2s/step - loss: 0.0891 - acc: 0.9148 - val_loss: 0.1534 - val_acc: 0.8131\n",
      "Epoch 68/100\n",
      "391/391 [==============================] - 694s 2s/step - loss: 0.0889 - acc: 0.9153 - val_loss: 0.1485 - val_acc: 0.8245\n",
      "Epoch 69/100\n",
      "391/391 [==============================] - 652s 2s/step - loss: 0.0883 - acc: 0.9159 - val_loss: 0.1599 - val_acc: 0.8110\n",
      "Epoch 70/100\n",
      "391/391 [==============================] - 991s 3s/step - loss: 0.0878 - acc: 0.9162 - val_loss: 0.1541 - val_acc: 0.8195\n",
      "Epoch 71/100\n",
      "391/391 [==============================] - 1366s 3s/step - loss: 0.0877 - acc: 0.9167 - val_loss: 0.1524 - val_acc: 0.8189\n",
      "Epoch 72/100\n",
      "391/391 [==============================] - 822s 2s/step - loss: 0.0871 - acc: 0.9176 - val_loss: 0.1544 - val_acc: 0.8172\n",
      "Epoch 73/100\n",
      "391/391 [==============================] - 614s 2s/step - loss: 0.0860 - acc: 0.9186 - val_loss: 0.1512 - val_acc: 0.8240\n",
      "Epoch 74/100\n",
      "391/391 [==============================] - 625s 2s/step - loss: 0.0867 - acc: 0.9172 - val_loss: 0.1477 - val_acc: 0.8272\n",
      "Epoch 75/100\n",
      "391/391 [==============================] - 630s 2s/step - loss: 0.0855 - acc: 0.9198 - val_loss: 0.1480 - val_acc: 0.8223\n",
      "Epoch 76/100\n",
      "391/391 [==============================] - 631s 2s/step - loss: 0.0838 - acc: 0.9229 - val_loss: 0.1532 - val_acc: 0.8207\n",
      "Epoch 77/100\n",
      "391/391 [==============================] - 625s 2s/step - loss: 0.0858 - acc: 0.9206 - val_loss: 0.1505 - val_acc: 0.8217\n",
      "Epoch 78/100\n",
      "391/391 [==============================] - 622s 2s/step - loss: 0.0848 - acc: 0.9205 - val_loss: 0.1545 - val_acc: 0.8245\n",
      "Epoch 79/100\n",
      "391/391 [==============================] - 616s 2s/step - loss: 0.0844 - acc: 0.9210 - val_loss: 0.1467 - val_acc: 0.8262\n",
      "Epoch 80/100\n",
      "391/391 [==============================] - 615s 2s/step - loss: 0.0829 - acc: 0.9253 - val_loss: 0.1520 - val_acc: 0.8232\n",
      "Epoch 81/100\n",
      "391/391 [==============================] - 614s 2s/step - loss: 0.0821 - acc: 0.9255 - val_loss: 0.1486 - val_acc: 0.8246\n",
      "Epoch 82/100\n",
      "391/391 [==============================] - 623s 2s/step - loss: 0.0828 - acc: 0.9241 - val_loss: 0.1501 - val_acc: 0.8246\n",
      "Epoch 83/100\n",
      "391/391 [==============================] - 617s 2s/step - loss: 0.0816 - acc: 0.9255 - val_loss: 0.1567 - val_acc: 0.8183\n",
      "Epoch 84/100\n",
      "391/391 [==============================] - 617s 2s/step - loss: 0.0805 - acc: 0.9275 - val_loss: 0.1596 - val_acc: 0.8109\n",
      "Epoch 85/100\n",
      "391/391 [==============================] - 618s 2s/step - loss: 0.0805 - acc: 0.9277 - val_loss: 0.1528 - val_acc: 0.8260\n",
      "Epoch 86/100\n",
      "391/391 [==============================] - 614s 2s/step - loss: 0.0794 - acc: 0.9304 - val_loss: 0.1493 - val_acc: 0.8251\n",
      "Epoch 87/100\n",
      "391/391 [==============================] - 614s 2s/step - loss: 0.0797 - acc: 0.9286 - val_loss: 0.1527 - val_acc: 0.8215\n",
      "Epoch 88/100\n",
      "391/391 [==============================] - 618s 2s/step - loss: 0.0785 - acc: 0.9306 - val_loss: 0.1497 - val_acc: 0.8254\n",
      "Epoch 89/100\n",
      "391/391 [==============================] - 621s 2s/step - loss: 0.0794 - acc: 0.9290 - val_loss: 0.1562 - val_acc: 0.8170\n",
      "Epoch 90/100\n",
      "391/391 [==============================] - 608s 2s/step - loss: 0.0787 - acc: 0.9287 - val_loss: 0.1542 - val_acc: 0.8259\n",
      "Epoch 91/100\n",
      "391/391 [==============================] - 615s 2s/step - loss: 0.0788 - acc: 0.9304 - val_loss: 0.1536 - val_acc: 0.8235\n",
      "Epoch 92/100\n",
      "391/391 [==============================] - 1833s 5s/step - loss: 0.0778 - acc: 0.9313 - val_loss: 0.1505 - val_acc: 0.8257\n",
      "Epoch 93/100\n",
      "391/391 [==============================] - 1790s 5s/step - loss: 0.0769 - acc: 0.9333 - val_loss: 0.1510 - val_acc: 0.8230\n",
      "Epoch 94/100\n",
      "391/391 [==============================] - 1787s 5s/step - loss: 0.0760 - acc: 0.9335 - val_loss: 0.1466 - val_acc: 0.8319\n",
      "Epoch 95/100\n",
      "391/391 [==============================] - 1786s 5s/step - loss: 0.0772 - acc: 0.9329 - val_loss: 0.1487 - val_acc: 0.8273\n",
      "Epoch 96/100\n",
      "391/391 [==============================] - 1718s 4s/step - loss: 0.0766 - acc: 0.9321 - val_loss: 0.1535 - val_acc: 0.8189\n",
      "Epoch 97/100\n",
      "391/391 [==============================] - 1784s 5s/step - loss: 0.0761 - acc: 0.9350 - val_loss: 0.1525 - val_acc: 0.8239\n",
      "Epoch 98/100\n",
      "391/391 [==============================] - 1792s 5s/step - loss: 0.0755 - acc: 0.9335 - val_loss: 0.1509 - val_acc: 0.8291\n",
      "Epoch 99/100\n",
      "391/391 [==============================] - 1705s 4s/step - loss: 0.0748 - acc: 0.9366 - val_loss: 0.1607 - val_acc: 0.8155\n",
      "Epoch 100/100\n",
      "391/391 [==============================] - 1482s 4s/step - loss: 0.0745 - acc: 0.9355 - val_loss: 0.1520 - val_acc: 0.8261\n"
     ]
    }
   ],
   "source": [
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(x_test, y_test),\n",
    "        shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by dataset std\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in 0 to 180 degrees\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally\n",
    "        height_shift_range=0.1,  # randomly shift images vertically\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(\n",
    "        datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "        epochs=epochs,\n",
    "        validation_data=(x_test, y_test),\n",
    "        workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_capsule_network_trained_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at /Users/i24009/Documents/kaggle/github/deep-learning/experiments-with-keras/saved_models/keras_cifar10_capsule_network_trained_model.h5 \n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
